# -*- coding: utf-8 -*-
"""Gradio api server.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yc0YGymGtbxoURgofUOjpuKRgWYOdhlU
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers accelerate

# @title Deploy Models using Gradio

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import os

# --- Configuration ---
MODEL1_PATH = "/content/drive/MyDrive/models/mbo_deberta"
MODEL2_PATH = "/content/drive/MyDrive/models/mbo_deberta_cg"

# --- Model Loading (Ensure this happens before running Gradio) ---
print("Loading models and tokenizers for Gradio...")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

def load_model_and_tokenizer(model_path, device):
    """Loads tokenizer and model from a given path."""
    try:
        # Check if path exists
        if not os.path.isdir(model_path):
             print(f"Error: Directory not found: {model_path}")
             return None, None
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        model.to(device)
        model.eval() # Set model to evaluation mode
        print(f"Successfully loaded model and tokenizer from: {model_path}")
        return tokenizer, model
    except Exception as e:
        print(f"Error loading model from {model_path}: {e}")
        # Attempt to provide more specific feedback if possible
        if "config.json" not in str(e) and "tokenizer_config.json" not in str(e):
             print(f"-> Check if the path is correct and contains model files (like config.json, pytorch_model.bin, tokenizer_config.json).")
        return None, None

# Load models and tokenizers specifically for Gradio use
# (Avoids potential conflicts if variables were reused/modified elsewhere)
gr_tokenizer1, gr_model1 = load_model_and_tokenizer(MODEL1_PATH, device)
gr_tokenizer2, gr_model2 = load_model_and_tokenizer(MODEL2_PATH, device)

# --- Prediction Functions for Gradio ---
def predict_sentiment_gradio(text, tokenizer, model, device):
    """Performs prediction and returns a dictionary suitable for Gradio output."""
    if not text or not tokenizer or not model:
        return {"error": "Missing text, tokenizer, or model"}

    try:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        inputs = {key: val.to(device) for key, val in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)

        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=-1) # Calculate probabilities
        predicted_class_id = torch.argmax(logits, dim=-1).item()
        predicted_label = model.config.id2label[predicted_class_id]
        confidence = probabilities[0, predicted_class_id].item() # Get confidence of the predicted class

        # Return a dictionary (good for gr.JSON or structured gr.Label)
        return {
            "input_text": text,
            "predicted_label": predicted_label,
            "predicted_class_id": predicted_class_id,
            "confidence": f"{confidence:.4f}",
            # "all_probabilities": {model.config.id2label[i]: prob.item() for i, prob in enumerate(probabilities[0])} # Optional: all probs
        }
    except Exception as e:
        print(f"Error during Gradio prediction: {e}")
        return {"error": str(e)}

# --- Create Wrapper Functions for Each Model ---
def predict_model1_gradio(text):
    if not gr_model1 or not gr_tokenizer1:
         # Use Gradio's exception handling for clearer UI feedback
         raise gr.Error(f"Model 1 ({MODEL1_PATH}) failed to load. Cannot predict.")
    return predict_sentiment_gradio(text, gr_tokenizer1, gr_model1, device)

def predict_model2_gradio(text):
    if not gr_model2 or not gr_tokenizer2:
        raise gr.Error(f"Model 2 ({MODEL2_PATH}) failed to load. Cannot predict.")
    return predict_sentiment_gradio(text, gr_tokenizer2, gr_model2, device)

# --- Create Gradio Interfaces ---
print("\nSetting up Gradio interface...")

# Define interface for Model 1
iface1 = gr.Interface(
    fn=predict_model1_gradio,
    inputs=gr.Textbox(lines=2, placeholder="Enter review text here for Model 1..."),
    # outputs=gr.Label(num_top_classes=3), # Shows top classes and probabilities
    outputs=gr.JSON(), # Display the full dictionary output
    title="Sentiment Analysis - Model 1 (mbo_deberta)",
    description="Enter text to classify its sentiment using the first model.",
    examples=[["I'm good with this product"], ["It works well with an i pod."]]
)

# Define interface for Model 2
iface2 = gr.Interface(
    fn=predict_model2_gradio,
    inputs=gr.Textbox(lines=2, placeholder="Enter review text here for Model 2..."),
    # outputs=gr.Label(num_top_classes=3),
    outputs=gr.JSON(),
    title="Sentiment Analysis - Model 2 (mbo_deberta_cg)",
    description="Enter text to classify its sentiment using the second model.",
    examples=[["All looks well"], ["good"]]
)

# Combine interfaces into Tabs
demo = gr.TabbedInterface(
    [iface1, iface2],
    ["Model 1: mbo_deberta", "Model 2: mbo_deberta_cg"]
)

# --- Launch Gradio App ---
print("Launching Gradio app... Wait for the public URL.")
# share=True creates the public link. debug=True gives more console output.
demo.launch(share=True, debug=True)

print("Gradio app launch initiated. Check the output above for the URLs.")
print("The cell will keep running to keep the Gradio app alive.")
print("Use the public URL (ending in .gradio.live) to access the interface.")

pip install gradio





































from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer

# Replace with the actual path to your model files
model_path = "/content/drive/MyDrive/mbo_deberta" # Or "/content/drive/My Drive/my_model_files/"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load the model
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Move the model to the GPU if a GPU is available
import torch
if torch.cuda.is_available():
    model.to('cuda')
    print("Model moved to GPU.")
else:
    print("GPU not available. Model will run on CPU.")

!pip install gradio

from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer

# Replace with the actual path to your model files
model_path = "/content/drive/MyDrive/mbo_deberta_cg" # Or "/content/drive/My Drive/my_model_files/"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load the model
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Move the model to the GPU if a GPU is available
import torch
if torch.cuda.is_available():
    model.to('cuda')
    print("Model moved to GPU.")
else:
    print("GPU not available. Model will run on CPU.")

# Example input text
input_text = "good"

# Tokenize the input text
inputs = tokenizer(input_text, return_tensors="pt")

# Move input tensors to the GPU if the model is on GPU
if torch.cuda.is_available():
    inputs = {key: val.to('cuda') for key, val in inputs.items()}

# Perform a forward pass
with torch.no_grad(): # Disable gradient calculation for inference
    outputs = model(**inputs)

# The 'outputs' variable now contains the model's output.
# The structure of outputs depends on the specific model architecture.
# For example, for a base model, it might be a BaseModelOutput object.
print("Model output structure:", type(outputs))
# print(outputs)

# If it's a text generation model, you would use model.generate() instead:
# For text generation, you might need a model class specifically for generation, like AutoModelForCausalLM
# from transformers import AutoModelForCausalLM
# model = AutoModelForCausalLM.from_pretrained(model_path)
# if torch.cuda.is_available():
#     model.to('cuda')
# generated_output = model.generate(**inputs, max_length=50)
# print("Generated text:", tokenizer.decode(generated_output[0], skip_special_tokens=True))

from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer

# Replace with the actual path to your model files
model_path = "/content/drive/MyDrive/mbo_deberta_cg" # Or "/content/drive/My Drive/my_model_files/"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load the model
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Move the model to the GPU if a GPU is available
import torch
if torch.cuda.is_available():
    model.to('cuda')
    print("Model moved to GPU.")
else:
    print("GPU not available. Model will run on CPU.")

# Example input text (a review)
input_text = "All looks well" # Or "This product is terrible and broke immediately."

# Tokenize the input text
inputs = tokenizer(input_text, return_tensors="pt")

# Move input tensors to the GPU if the model is on GPU
if torch.cuda.is_available():
    inputs = {key: val.to('cuda') for key, val in inputs.items()}

# Perform a forward pass
with torch.no_grad(): # Disable gradient calculation for inference
    outputs = model(**inputs)

# The outputs for a sequence classification model contain logits
logits = outputs.logits

# Get the predicted class index (0 or 1)
predicted_class_id = torch.argmax(logits, dim=-1).item()

# Get the label associated with the predicted index
predicted_label = model.config.id2label[predicted_class_id]

print(f"Review: \"{input_text}\"")
print(f"Predicted sentiment: {predicted_label}")

# You can also see the raw logits
# print("Logits:", logits)

import pandas as pd

# Replace 'your_dataset.csv' with the path to your dataset file
# Replace 'review_text' and 'class_label' with your column names
try:
    df = pd.read_csv("/content/drive/MyDrive/final.csv")
    print("Dataset loaded successfully.")
    print(f"Dataset shape: {df.shape}")
    print("Dataset columns:", df.columns.tolist())
    print("First 5 rows:\n", df.head())
except FileNotFoundError:
    print("Error: Dataset file not found. Please check the path.")
    # You might want to exit or handle this error appropriately
    exit() # Or sys.exit() if you import sys

# Define the mapping based on your model's output
label_mapping = {'LABEL_1': 0, 'LABEL_2': 1}

# Map the true labels in your DataFrame to numerical values
# Make sure the 'class_label' column contains exactly 'LABEL_1' or 'LABEL_2'
df['true_label_id'] = df['class'].map(label_mapping)

# Check if there were any labels that didn't match the mapping
if df['true_label_id'].isnull().any():
    print("Warning: Some true labels did not match the defined mapping.")
    print("Labels without mapping:", df[df['true_label_id'].isnull()]['class'].unique())

print("\nTrue labels mapped to numerical IDs.")
print("First 5 rows with mapped labels:\n", df.head())

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Replace with the actual path to your model files
model_path = "/content/drive/MyDrive/mbo_deberta_"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load the model specifically for sequence classification
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Move the model to the GPU if a GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(f"Model moved to {device}.")

# Get the label to ID mapping from the loaded model's config
# This is important to confirm the model's internal mapping
model_id_to_label = model.config.id2label
print(f"\nModel's internal ID to Label mapping: {model_id_to_label}")

# Verify your manual mapping matches the model's (should be 0: LABEL_1, 1: LABEL_2)
if model_id_to_label.get(0) != 'LABEL_1' or model_id_to_label.get(1) != 'LABEL_2':
    print("Warning: Model's internal ID to Label mapping does not match the expected LABEL_1 (0) and LABEL_2 (1).")
    print("Please check your model training or the label_mapping dictionary.")

predicted_class_ids = []

print("\nGenerating predictions for each review...")

# Process reviews in batches if your dataset is large to save memory
# For simplicity, processing one by one here
for index, row in df.iterrows():
    review_text = row['review']

    # Tokenize the input text
    inputs = tokenizer(review_text, return_tensors="pt", padding=True, truncation=True, max_length=512) # Added padding/truncation

    # Move input tensors to the correct device (GPU or CPU)
    inputs = {key: val.to(device) for key, val in inputs.items()}

    # Perform a forward pass
    with torch.no_grad():
        outputs = model(**inputs)

    # Get the logits and the predicted class index
    logits = outputs.logits
    predicted_class_id = torch.argmax(logits, dim=-1).item()

    predicted_class_ids.append(predicted_class_id)

print("Finished generating predictions.")

# Add predictions to the DataFrame (optional, but helpful for debugging)
df['predicted_label_id'] = predicted_class_ids
print("\nFirst 5 rows with predictions:\n", df.head())

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score

# Get the true and predicted labels as lists
true_labels = df['class'].tolist()
predicted_labels = df['predicted_label_id'].tolist()

accuracy = accuracy_score(true_labels, predicted_labels)
precision = precision_score(true_labels, predicted_labels, average='macro')  # Changed to 'macro'
recall = recall_score(true_labels, predicted_labels, average='macro')  # Changed to 'macro'
f1 = f1_score(true_labels, predicted_labels, average='macro')  # Changed to 'macro'
cm = confusion_matrix(true_labels, predicted_labels)

# ... (rest of your code) ...

print("\n--- Evaluation Metrics ---")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1 Score : {f1:.4f}")


# Calculate Confusion Matrix
# The confusion matrix is typically ordered as [[TN, FP], [FN, TP]]
# for binary classification where 0 is the negative class and 1 is the positive class.
# In your case: 0 is LABEL_1 (Negative), 1 is LABEL_2 (Positive)
cm = confusion_matrix(true_labels, predicted_labels)

# Extract TN, FP, FN, TP from the confusion matrix
# Ensure the confusion matrix has the expected shape for binary classification
if cm.shape == (2, 2):
    tn, fp, fn, tp = cm.ravel()
    print("\n--- Evaluation Metrics ---")
    print(f"Accuracy: {accuracy:.4f}")

    print(f"True Positives (TP): {tp}")    # Correctly predicted LABEL_2 (1)
    print(f"True Negatives (TN): {tn}")    # Correctly predicted LABEL_1 (0)
    print(f"False Positives (FP): {fp}")  # Predicted LABEL_2 (1) but was LABEL_1 (0)
    print(f"False Negatives (FN): {fn}")  # Predicted LABEL_1 (0) but was LABEL_2 (1)
    print("\nConfusion Matrix:")
    print(cm)
    # Confusion matrix structure:
    # [[True Negatives (correctly predicted 0), False Positives (predicted 1, was 0)],
    #  [False Negatives (predicted 0, was 1), True Positives (correctly predicted 1)]]

else:
    print("\nWarning: Confusion matrix shape is not as expected for binary classification.")
    print("Confusion Matrix:")
    print(cm)

